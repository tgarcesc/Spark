{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, QuantileDiscretizer, OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession.builder\\\n",
    "        .master('yarn')\\\n",
    "        .config(\"spark.driver.memory\", \"8g\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config(\"spark.yarn.executor.memoryOverhead\",\"8g\")\\\n",
    "        .config(\"spark.driver.memoryOverhead\",\"8g\")\\\n",
    "        .config(\"spark.sql.crossJoin.enabled\",\"true\")\\\n",
    "        .appName(\"CatAutMas_VF_tg\")\\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_time = time.time()\n",
    "\n",
    "spark = init_spark()\n",
    "\n",
    "exec_time = time.time() - ini_time\n",
    "print(\"Tiempo de Inicio de Spark: \" + str(datetime.timedelta(seconds=exec_time)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "appid = spark._sc.applicationId\n",
    "sparkui = \"http://schlboclomp0003.corporativo.cl.corp:8088/proxy/\" + appid\n",
    "#print(sparkui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una función para realizar el proceso de precategorización\n",
    "def multiDiscretizer(i, Buckets, data, varDep):\n",
    "    #generamos el escenario univariable (i) para la data de entrada para valores no nulos\n",
    "    \n",
    "    dataVar = data.select(varDep, i).filter(i+ \" IS NOT NULL AND \"+i+\" > \" + umbral )\n",
    "    #configuramos el QuantileDiscretizer\n",
    "    discretizer = QuantileDiscretizer(numBuckets=Buckets, inputCol=i, outputCol='Bin')\n",
    "    \n",
    "    try:\n",
    "        #Entrenamiento y aplicación del modelo\n",
    "        result = discretizer.fit(dataVar)\n",
    "        prebin = result.transform(dataVar)\n",
    "        \n",
    "        min1 = prebin.select(min(\"Bin\")).first()\n",
    "        if(min1[0] == 0.0):\n",
    "            prebin = prebin.withColumn(\"Bin\",prebin.Bin+1)\n",
    "        \n",
    "        return prebin\n",
    "    except:\n",
    "        #Imprimimos si hubo una variable con problemas de ejecución.\n",
    "        print('Problemas con la variable: ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la función\n",
    "def LimitCateg(p):\n",
    "    if type(p)==pyspark.sql.dataframe.DataFrame:\n",
    "        try:\n",
    "            #-------------------- Creación de límites de Bins ------------------#\n",
    "            #Extraemos el nombre de la variable de la columna 1\n",
    "            variable = p.schema.names[1]\n",
    "            #---- Contrucción de LB, Ub y totales por Bin -----------------------------------------------------#\n",
    "            count_bin_max = p.groupBy(\"Bin\").agg(max(variable).alias(\"Ub\"))\n",
    "            count_bin_min = p.groupBy(\"Bin\").agg(min(variable).alias(\"Lb\"))\n",
    "            limitBin = count_bin_min.join(count_bin_max,[\"Bin\"],\"full\")\n",
    "            #Se realiza un conteo de observaciones por bin\n",
    "            Count = p.groupBy(f.col('Bin')).agg(f.count('Bin').alias(\"TOTALES\"))\n",
    "            limitCount = limitBin.join(Count,[\"Bin\"],\"full\").orderBy(\"Bin\")\n",
    "            return limitCount\n",
    "        except:\n",
    "            print('Problemas con la variable: ', variable)\n",
    "            #return p\n",
    "    else:\n",
    "        print('######## VARIABLE NO PROCESADA: ######## ', type(p), p)\n",
    "        #return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una función para realizar el proceso de precategorización\n",
    "def MultiClasificacion(p):\n",
    "    #generamos el escenario univariable (i) para la data de entrada\n",
    "    if type(p)==pyspark.sql.dataframe.DataFrame:\n",
    "        try:\n",
    "            #-------------------- Recategorizacion ------------------#\n",
    "            #Extraemos el nombre de la variable de la columna 1\n",
    "            variable = p.schema.names[1]\n",
    "            #Borramos la columna que contiene las observaciones de la variable\n",
    "            p = p.drop(variable)\n",
    "            #Asginamos el nombre de la variable a la columna de bins\n",
    "            clasif = p.withColumnRenamed(\"Bin\", variable)\n",
    "            return clasif\n",
    "        except:\n",
    "            print('Problemas con la variable: ', p.schema.names)\n",
    "            #return p\n",
    "    else:\n",
    "        print('######## VARIABLE NO PROCESADA: ######## ', type(p), p,)\n",
    "        #return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiDecisionTree(t):\n",
    "    \n",
    "    #Arbol para Variables Numéricas Continuas:\n",
    "    if type(t)==pyspark.sql.dataframe.DataFrame:\n",
    "        varInd = t.schema.names[1]\n",
    "        try:\n",
    "            \n",
    "            dt = DecisionTreeClassifier(labelCol=varDep, \n",
    "                                        featuresCol=\"features\", \n",
    "                                        impurity='entropy',\n",
    "                                        maxDepth=3, \n",
    "                                        maxBins=20, \n",
    "                                        minInstancesPerNode=20, #Número mínimo de elementos por bin\n",
    "                                        #minInstancesPerNode=int(maxObs*0.05), \n",
    "                                        minInfoGain=0.0)\n",
    "\n",
    "            varInd = t.schema.names[1]\n",
    "            \n",
    "            #-------------------------------------------------\n",
    "            ## Transformaciones de la data para el arbol\n",
    "            #-------------------------------------------------\n",
    "            vector_assembler = VectorAssembler(inputCols=[varInd],outputCol=\"features\")\n",
    "            pipeline= Pipeline(stages=[vector_assembler, dt])\n",
    "            model = pipeline.fit(t)\n",
    "            predictions = model.transform(t)\n",
    "\n",
    "            ##------------------------------------------------\n",
    "            ##Armado de tabla de clasificaciones\n",
    "            #-------------------------------------------------\n",
    "            tablaBruta = predictions.select(varInd, \"features\", \"rawPrediction\", \"prediction\").distinct().orderBy(\"features\")\n",
    "            w = Window.orderBy(tablaBruta.rawPrediction)\n",
    "            Segmentacion_label = tablaBruta.withColumn(\"BIN_Random_\"+varInd, dense_rank().over(w))\n",
    "            \n",
    "            salida_df = Segmentacion_label.select(varInd, \"rawPrediction\", \"BIN_Random_\"+varInd)\n",
    "            this_df = salida_df.select(salida_df[varInd], salida_df[\"BIN_Random_\"+varInd]).groupBy(\"BIN_Random_\"+varInd).agg(max(salida_df[varInd]).alias(varInd))\n",
    "            final_df = this_df.withColumn(\"BIN_SANO\", f.row_number().over(Window.partitionBy().orderBy(varInd))).select(\"BIN_SANO\", varInd, \"BIN_Random_\"+varInd)\n",
    "            \n",
    "            tablaBruta = salida_df.join(final_df, salida_df[\"BIN_Random_\"+varInd] == final_df[\"BIN_Random_\"+varInd], how = \"inner\")\\\n",
    "                                  .select(final_df[\"BIN_SANO\"].alias(\"BIN_\"+varInd), salida_df[varInd])\n",
    "            salida =t.join(tablaBruta, on = [varInd], how = 'left')\n",
    "            salida = salida.drop(\"features\", \"rawPrediction\")\n",
    "            \n",
    "            return salida\n",
    "\n",
    "        except Exception as e:\n",
    "            \n",
    "            print('Problemas con la variable: ', varInd)\n",
    "            print('ERROR: ', e)\n",
    "            #return t\n",
    "    else:\n",
    "        print('######## VARIABLE NO PROCESADA: ######## ', type(t), t)\n",
    "        #return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## DECLARACION DE FUNCION DE ARBOL DE CLASIFICACION PARA VARIABLES CATEGORICAS\n",
    "def multiDecisionTreeCateg(t):\n",
    "    \n",
    "    if type(t)==pyspark.sql.dataframe.DataFrame:\n",
    "        try:    \n",
    "            dt = DecisionTreeClassifier(labelCol=varDep, \n",
    "                                        featuresCol=\"features\", \n",
    "                                        #impurity='entropy',\n",
    "                                        maxDepth=3, \n",
    "                                        maxBins=20, \n",
    "                                        minInstancesPerNode=20, #Número mínimo de elementos por bin\n",
    "                                        #minInstancesPerNode=int(maxObs*0.05), \n",
    "                                        minInfoGain=0.0)\n",
    "\n",
    "            varInd = t.schema.names[1]\n",
    "            \n",
    "            #-------------------------------------------------\n",
    "            ## Transformaciones de la data para el arbol\n",
    "            #-------------------------------------------------\n",
    "            indexador = StringIndexer(inputCol=varInd, outputCol=\"variables\", stringOrderType=\"alphabetAsc\")\n",
    "            encoder = OneHotEncoderEstimator(inputCols=[\"variables\"],outputCols=[\"features\"], dropLast=False)\n",
    "            \n",
    "            pipeline= Pipeline(stages=[indexador, encoder, dt])\n",
    "            \n",
    "            model = pipeline.fit(t)\n",
    "            predictions = model.transform(t)\n",
    "\n",
    "            ##------------------------------------------------\n",
    "            ##Armado de tabla de clasificaciones\n",
    "            #-------------------------------------------------\n",
    "            \n",
    "            tablaBruta = predictions.select(varInd, \"features\", \"rawPrediction\", \"prediction\").distinct().orderBy(\"features\")\n",
    "            w = Window.orderBy(tablaBruta.rawPrediction)\n",
    "            Segmentacion_label = tablaBruta.withColumn(\"BIN_Random_\"+varInd, dense_rank().over(w))\n",
    "            \n",
    "            salida_df = Segmentacion_label.select(varInd, \"rawPrediction\", \"BIN_Random_\"+varInd)\n",
    "            this_df = salida_df.select(salida_df[varInd], salida_df[\"BIN_Random_\"+varInd]).groupBy(\"BIN_Random_\"+varInd).agg(max(salida_df[varInd]).alias(varInd))\n",
    "            final_df = this_df.withColumn(\"BIN_SANO\", f.row_number().over(Window.partitionBy().orderBy(varInd))).select(\"BIN_SANO\", varInd, \"BIN_Random_\"+varInd)\n",
    "            \n",
    "            tablaBruta = salida_df.join(final_df, salida_df[\"BIN_Random_\"+varInd] == final_df[\"BIN_Random_\"+varInd], how = \"inner\")\\\n",
    "                                  .select(final_df[\"BIN_SANO\"].alias(\"BIN_\"+varInd), salida_df[varInd])\n",
    "            salida =t.join(tablaBruta, on = [varInd], how = 'left')\n",
    "            salida = salida.drop(\"features\", \"rawPrediction\")\n",
    "            \n",
    "            return salida\n",
    "\n",
    "        except BaseException as e:\n",
    "            print('Problemas con la variable: ', varInd)\n",
    "            print('ERROR: ', e)\n",
    "            #return t\n",
    "    else:\n",
    "        print('######## VARIABLE NO PROCESADA: ######## ', type(t), t)\n",
    "        #return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LimitCategBinning(p):\n",
    "    if type(p)==pyspark.sql.dataframe.DataFrame:\n",
    "        try:\n",
    "            #-------------------- Creación de límites de Bins ------------------#\n",
    "            #Extraemos el nombre de la variable de la columna 1\n",
    "            varInd = p.schema.names[0]\n",
    "            #---- Contrucción de LB, Ub y totales por Bin -----------------------------------------------------#\n",
    "            #---- Contrucción de LB, Ub y totales por Bin -----------------------------------------------------#\n",
    "            count_bin = p.groupBy(\"Bin_\"+varInd).agg(min(varInd).alias(\"Lb\"), max(varInd)\\\n",
    "                                                .alias(\"Ub\"),f.count('Bin_'+varInd).alias(\"TOTALES\"))\\\n",
    "                                                .sort('Bin_'+varInd)\n",
    "            \n",
    "            return count_bin\n",
    "        except:\n",
    "            print('Problemas con la variable: ', varInd)\n",
    "            #return p\n",
    "    else:\n",
    "        print('######## VARIABLE NO PROCESADA: ######## ', type(p), p)\n",
    "        #return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limitesOriginales(l, limitCateg):\n",
    "    if type(l)==pyspark.sql.dataframe.DataFrame:\n",
    "        try:\n",
    "            #-------- Extraccion de Variable--------------#\n",
    "            varInd = l.schema.names[0].split(\"Bin_\")[1]\n",
    "            index = dfVarsNum.schema.names.index(varInd)\n",
    "            q = limitCateg[index]\n",
    "            #------- Proceso de Joins ---------------------#\n",
    "            #Cambio de nombres de columnas\n",
    "            LimBin=l.withColumnRenamed(\"Lb\",\"Lb_1\")\\\n",
    "                    .withColumnRenamed(\"Ub\",\"Ub_1\")\\\n",
    "                    .withColumnRenamed(\"TOTALES\",\"TOTALES_1\")\n",
    "            LimBin = LimBin.join(q,LimBin.Lb_1==q.Bin,\"left\")\n",
    "            LimBin = LimBin.withColumnRenamed(\"Lb\",\"Lb_2\")\\\n",
    "                           .withColumnRenamed(\"Ub\",\"Ub_2\")\\\n",
    "                           .withColumnRenamed(\"TOTALES\",\"TOTALES_2\")\\\n",
    "                           .withColumnRenamed(\"Bin\",\"Bin_1\")\n",
    "            LimBin =  LimBin.join(q, LimBin.Ub_1==q.Bin, \"left\")\n",
    "            LimBin=LimBin.drop(\"Lb_1\").drop(\"Ub_1\").drop(\"Bin_1\")\\\n",
    "                         .drop(\"Ub_2\").drop(\"TOTALES_2\")\\\n",
    "                         .drop(\"Lb\").drop(\"Bin\")\\\n",
    "                         .drop(\"TOTALES\")\n",
    "            LimBin=LimBin.withColumnRenamed(\"Lb_2\",\"Lb\")\\\n",
    "                         .withColumnRenamed(\"TOTALES_1\",\"TOTALES\").orderBy(\"Bin_\"+varInd)\n",
    "            return LimBin\n",
    "        except Exception as e:\n",
    "            print('Problemas con la variable: ', varInd)\n",
    "            print('ERROR: ', e)\n",
    "    else:\n",
    "        print('######## VARIABLE NO PROCESADA: ######## ', type(l), l)\n",
    "        #return l  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MultiAsignacionCategorias(p, LimitesOriginales, dfVarsNum):   \n",
    "    \n",
    "    cotas = 1\n",
    "    if type(p)==pyspark.sql.dataframe.DataFrame:\n",
    "        try:\n",
    "            #-------- Extraccion de Variable--------------#\n",
    "            varInd = p.schema.names[1]\n",
    "            index = dfVarsNum.schema.names.index(varInd)\n",
    "            q = LimitesOriginales[index]\n",
    "            binvarInd = \"Bin_\"+varInd  \n",
    "            if(cotas != 1):                           \n",
    "                df_final = p.join(q, (((p[varInd] < q[\"Ub\"]) & (p[varInd] >= q[\"Lb\"])) | ((p[varInd] == q[\"Ub\"]) & (p[varInd] == q[\"Lb\"]))), how = \"left\")\\\n",
    "                                .select(p[\"*\"], q[binvarInd].alias(\"PRE_\" + varInd))\n",
    "            else:\n",
    "                df_final = p.join(q, (((p[varInd] < q[\"Ub\"]) & (p[varInd] >= q[\"Lb\"])) | ((p[varInd] == q[\"Ub\"]) & (p[varInd] == q[\"Lb\"]))), how = \"left\")\\\n",
    "                                .select(p[\"*\"], q[binvarInd].alias(\"PRE_\" + varInd), q[\"Lb\"], q[\"Ub\"])          \n",
    "            return df_final\n",
    "        except Exception as e:\n",
    "            print('Problemas con la variable: ', varInd)\n",
    "            print('ERROR: ', e)\n",
    "    else:\n",
    "        print('######## VARIABLE NO PROCESADA: ######## ', type(p), p)\n",
    "        #return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiIvWoe(n, data_l):\n",
    "    \n",
    "    if type(n)==pyspark.sql.dataframe.DataFrame:\n",
    "        try:\n",
    "            varInd = n.schema.names[1]\n",
    "            pre_var =\"PRE_\"+varInd\n",
    "            var_lb = \"lb\"\n",
    "            var_ub = \"ub\"\n",
    "            #---------------------------------------------------------------------------------------------------------------------------#\n",
    "            newdataX = n.select(varInd, pre_var, var_lb, var_ub, varDep)\\\n",
    "                        .groupBy(pre_var, var_lb, var_ub)\\\n",
    "                        .agg(count(varDep).alias(\"count\"), (count(n[varDep])-sum(n[varDep])).alias(\"good\"), sum(n[varDep]).alias(\"bad\"))\\\n",
    "                        .withColumnRenamed(pre_var, \"bin\").withColumn(\"variable\", lit(varInd))\n",
    "            \n",
    "            ############ \n",
    "            df_flags = data_l.select(varDep, varInd).where(varInd + \" <= \" + umbral).agg(count(varInd).alias(\"count\"), \\\n",
    "                                                                        (count(data_l[varDep])-sum(data_l[varDep])).alias(\"good\"), \\\n",
    "                                                                        sum(data_l[varDep]).alias(\"bad\"), min(varInd).alias(\"lb\"), \\\n",
    "                                                                        max(varInd).alias(\"ub\"))\\\n",
    "                                                                        .withColumn(\"variable\", lit(varInd))\n",
    "            if(df_flags.count() > 0):\n",
    "                df_flags = df_flags.withColumn(\"bin\", lit('NA')).select(\"bin\",\"lb\", \"ub\", \"count\", \"good\", \"bad\", \"variable\") \n",
    "                newdataX = newdataX.union(df_flags) \n",
    "            \n",
    "            ########\n",
    "\n",
    "\n",
    "            newdataX2 = newdataX.select(\"variable\", \"good\", \"bad\").groupBy(\"variable\")\\\n",
    "                            .agg(sum(newdataX[\"good\"]).alias(\"total_buenos\"), sum(newdataX[\"bad\"]).alias(\"total_malos\"))\n",
    "\n",
    "            postdata = newdataX.join(newdataX2, newdataX[\"variable\"] == newdataX2[\"variable\"], how = 'inner')\\\n",
    "                                .select(newdataX[\"variable\"], newdataX[\"bin\"], newdataX[\"lb\"], newdataX[\"ub\"], \\\n",
    "                                newdataX[\"count\"], newdataX[\"good\"], newdataX[\"bad\"], (newdataX[\"good\"]/newdataX2[\"total_buenos\"]).alias(\"good_distr\"), \\\n",
    "                                (newdataX[\"bad\"]/newdataX2[\"total_malos\"]).alias(\"bad_distr\"), (newdataX[\"bad\"]/newdataX[\"count\"]).alias(\"badprob\"))\n",
    "\n",
    "            postdata = postdata.withColumn(\"woe\", log((postdata.good_distr/postdata.bad_distr)))\\\n",
    "                               .withColumn(\"iv\", (postdata.good_distr-postdata.bad_distr)*log(postdata.good_distr/postdata.bad_distr))\n",
    "\n",
    "            sumas = postdata.select(\"variable\", \"iv\").groupBy(\"variable\").agg(sum(postdata[\"iv\"]).alias(\"iv\"))\n",
    "\n",
    "            df3 = postdata.join(sumas, postdata[\"variable\"] == sumas[\"variable\"], how = \"inner\")\\\n",
    "                            .select(postdata[\"variable\"], postdata[\"bin\"], postdata[\"lb\"], postdata[\"ub\"], postdata[\"count\"], \\\n",
    "                            postdata[\"good\"], postdata[\"bad\"], round(postdata[\"good_distr\"], 5).alias(\"good_distr\"), round(postdata[\"bad_distr\"], 5).alias(\"bad_distr\"), round(postdata[\"badprob\"], 5).alias(\"badprob\"), \\\n",
    "                           round(postdata[\"woe\"], 5).alias(\"woe\"), round(postdata[\"iv\"], 5).alias(\"iv\"), round(sumas[\"iv\"], 5).alias(\"iv_total\"))\n",
    "            \n",
    "            #max1 = df3.select(max(\"ub\")).first()\n",
    "            #min1 = df3.select(min(\"lb\")).first()\n",
    "            #df3 = df3.withColumn(\"ub\", when(df3[\"ub\"] == max1[0],  lit('Infinity') ).otherwise(df3[\"ub\"]))\\\n",
    "            #            .withColumn(\"lb\", when(df3[\"lb\"] == min1[0], lit('-Infinity') ).otherwise(df3[\"lb\"]))\n",
    "            \n",
    "            return df3\n",
    "        except BaseException as e:\n",
    "            print('Problemas con la variable: ', varInd)\n",
    "            print('ERROR: ', e)\n",
    "    else:\n",
    "        print('######## VARIABLE NO PROCESADA: ######## ', type(n), n)\n",
    "        #return n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se define ensamblador de lista de data frames de variables categoricas\n",
    "def ensambladorC(i, dataC, varDep):\n",
    "    dataVar = dataC.select(varDep, i).filter(i+ \" IS NOT NULL\")\n",
    "    return dataVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiIvWoeCategorico(n):\n",
    "    \n",
    "    if type(n)==pyspark.sql.dataframe.DataFrame:\n",
    "        try:\n",
    "            varInd = n.schema.names[0]\n",
    "            pre_var =\"BIN_\"+varInd\n",
    "            #---------------------------------------------------------------------------------------------------------------------------#\n",
    "            newdataX = n.select(varInd, varDep, pre_var)\\\n",
    "                        .groupBy(pre_var)\\\n",
    "                        .agg(count(varDep).alias(\"count\"), (count(n[varDep])-sum(n[varDep])).alias(\"good\"), sum(n[varDep]).alias(\"bad\"))\\\n",
    "                        .withColumnRenamed(pre_var, \"bin\").withColumn(\"variable\", lit(varInd))\n",
    "            \n",
    "            ##-------------->\n",
    "            \n",
    "            newdataX2 = newdataX.select(\"variable\", \"good\", \"bad\").groupBy(\"variable\")\\\n",
    "                            .agg(sum(newdataX[\"good\"]).alias(\"total_buenos\"), sum(newdataX[\"bad\"]).alias(\"total_malos\"))\n",
    "            \n",
    "            postdata = newdataX.join(newdataX2, newdataX[\"variable\"] == newdataX2[\"variable\"], how = 'inner')\\\n",
    "                                .select(newdataX[\"variable\"], newdataX[\"bin\"], \\\n",
    "                                newdataX[\"count\"], newdataX[\"good\"], newdataX[\"bad\"], (newdataX[\"good\"]/newdataX2[\"total_buenos\"]).alias(\"good_distr\"), \\\n",
    "                                (newdataX[\"bad\"]/newdataX2[\"total_malos\"]).alias(\"bad_distr\"), (newdataX[\"bad\"]/newdataX[\"count\"]).alias(\"badprob\"))\n",
    "\n",
    "            postdata = postdata.withColumn(\"woe\", log((postdata.good_distr/postdata.bad_distr)))\\\n",
    "                               .withColumn(\"iv\", (postdata.good_distr-postdata.bad_distr)*log(postdata.good_distr/postdata.bad_distr))\n",
    "\n",
    "            sumas = postdata.select(\"variable\", \"iv\").groupBy(\"variable\").agg(sum(postdata[\"iv\"]).alias(\"iv\"))\n",
    "\n",
    "            df3 = postdata.join(sumas, postdata[\"variable\"] == sumas[\"variable\"], how = \"inner\")\\\n",
    "                            .select(postdata[\"variable\"], postdata[\"bin\"], postdata[\"count\"], \\\n",
    "                            postdata[\"good\"], postdata[\"bad\"], round(postdata[\"good_distr\"], 5).alias(\"good_distr\"), round(postdata[\"bad_distr\"], 5).alias(\"bad_distr\"), round(postdata[\"badprob\"], 5).alias(\"badprob\"), \\\n",
    "                           round(postdata[\"woe\"], 5).alias(\"woe\"), round(postdata[\"iv\"], 5).alias(\"iv\"), round(sumas[\"iv\"], 5).alias(\"iv_total\"))\n",
    "            df3 = df3.withColumn(\"lb\", lit(\"NA\")).withColumn(\"ub\", lit(\"NA\"))\n",
    "            \n",
    "            return df3\n",
    "        except BaseException as e:\n",
    "            print('Problemas con la variable: ', varInd)\n",
    "            print('ERROR: ', e)\n",
    "    else:\n",
    "        print('######## VARIABLE NO PROCESADA: ######## ', type(n), n)\n",
    "        #return n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "####                       -> CONFIGURACION <-                                ####\n",
    "##################################################################################\n",
    "######################## DATOS A USAR ############################################\n",
    "##################################################################################\n",
    "\n",
    "#########Importamos la data que contiene las variables a utilizar################\n",
    "#\n",
    "#df_master = spark.table(\"greporting_riesgos.mb_fuga_rrhh_vars_iv\")\n",
    "df_master = spark.table(\"grsg_modelos.data_ok\")\n",
    "#DictCateg = spark.table(\"grsg_modelos.aci_fuga\")\n",
    "DictCateg = spark.table(\"greporting_riesgos.dicc_data_ok\")\n",
    "\n",
    "\n",
    "### ---> Caso especial  -> data_ok\n",
    "DictCateg = DictCateg.drop(\"IND_BM\")\n",
    "DictCateg = DictCateg.drop(\"ID\")\n",
    "### ----->\n",
    "\n",
    "\n",
    "#Variable Dependiente de estudio\n",
    "varDep='ind_bm'\n",
    "#varDep = 'ind_fuga_09'\n",
    "#Numero de separaciones para clasificacion de Pre_binning\n",
    "Buckets = 20\n",
    "#Creamos un pool de 10 hilos que corren en paralelo\n",
    "pool = ThreadPool(10)\n",
    "#Umbral para variables especiales\n",
    "#Para no utilizar valores especiales, dejar umbral como un valor extremadamente negativo. ej: \n",
    "umbral = '-777770'\n",
    "#--------------------------------------------------------------------------------#\n",
    "#Imprimimos la lista\n",
    "aci_fuga = DictCateg.rdd.map(lambda x: x.name)\n",
    "#recolectar los valores del rdd en una lista\n",
    "Variables = aci_fuga.collect()\n",
    "#procesar el encode de la lista\n",
    "Variables = [s.encode('ascii') for s in Variables]\n",
    "#print(Variables)\n",
    "#--------------------------------------------------------------------------------#\n",
    "#Extraemos el subset de datos utilizados para el estudio\n",
    "#ASignamos el nombre de la variable dependiente\n",
    "#Añadimos a la lista Variables, la variable dependiente\n",
    "Variables.append(varDep)\n",
    "#Creamos un DataFrame que contiene el total de variables para el estudio\n",
    "dfVars = df_master.select([col for col in Variables])\n",
    "#---------------------------------------------------------------------------------#\n",
    "#Extraemos las variables de la columna names que tengan el valor de type = 'Char' a través de RDD\n",
    "names = DictCateg.select(\"name\").where(\"type_='Char'\").rdd.map(lambda x: x.name)\n",
    "categoricas = names.collect()\n",
    "#Añadimos la variable dependiente a la lista\n",
    "categoricas.append(varDep)\n",
    "#Codificamos la lista en el caso de que no venga en utf-8\n",
    "categoricas = [s.encode('ascii') for s in categoricas]\n",
    "#Extraemos las variables de la columna names que tengan el valor de type = 'Num' a través de RDD\n",
    "names = DictCateg.select(\"name\").where(\"type_='Num'\").rdd.map(lambda x: x.name)\n",
    "numericas = names.collect()\n",
    "numericas.append(varDep)\n",
    "#Codificamos la lista en el caso de que no venga en utf-8\n",
    "numericas = [s.encode('ascii') for s in numericas]\n",
    "\n",
    "\n",
    "#########################CASO ESPECIAL DATA OK    ##################################\n",
    "numericas.remove(\"IND_BM\")#Variable dependiente se almacena en otra parte. sacar del diccionario\n",
    "numericas.remove(\"ID\")#Identificador que no entrega valor al estudio. sacar del diccionario\n",
    "numericas.remove(\"CHAR052\")#Variable mal formateada de origen\n",
    "numericas.remove(\"ldn_pas_cta_cte_mn_sm\")#Variable mal formateada de origen\n",
    "####################################################################################\n",
    "#########################CASO ESPECIAL FUGA     ####################################\n",
    "#numericas.remove(\"desempenio\")#Variable viene con solo NULLs\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "#Extracción de DataFrame de variables categóricas\n",
    "dfVarsCateg = dfVars.select([col for col in categoricas])\n",
    "#Extracción de DataFrame de variables Numéricas\n",
    "dfVarsNum = dfVars.select([col for col  in numericas])\n",
    "\n",
    "#Creamos un lista de parámetros que contiene la lista de variables excepto la variable dependiente\n",
    "parameters = numericas[:len(numericas)-2]\n",
    "parametersC = categoricas[:len(categoricas)-2]\n",
    "#DAta NUMERICA final a usar:\n",
    "data = dfVarsNum\n",
    "#DAta CATEGORICA final a usar:\n",
    "dataC = dfVarsCateg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################             EJECUCIONES             ###########################\n",
    "################################################################################\n",
    "\n",
    "\n",
    "#\n",
    "#Se ejecuta prebinning para variables Numericas\n",
    "ini = time.time()\n",
    "preBinning = pool.map(lambda i: multiDiscretizer(i, Buckets, data, varDep), parameters)\n",
    "#total = time() - ini\n",
    "#print('Duracion Discretizer: ', total,'Segundos')\n",
    "#print(\"\\n\")\n",
    "\n",
    "#\n",
    "#Se ejecuta funcion de asignacion de categorias obtenidas en prebinning a la data de entrada\n",
    "#ini00 = time()\n",
    "limitCateg = pool.map(lambda p:  LimitCateg(p), preBinning)\n",
    "#total = time() - ini00\n",
    "#print('Duracion limit Categ: ', total,' Segundos')\n",
    "#print(\"\\n\")\n",
    "\n",
    "#\n",
    "#Se ejecuta funcion de creacion de columna bins con clasificacion obtenida\n",
    "#ini0 = time()\n",
    "clasificacion = pool.map(lambda p:  MultiClasificacion(p), preBinning)\n",
    "#total0 = time() - ini0\n",
    "#print('Duracion multi clasificacion: ', total0,' Segundos')\n",
    "#print(\"\\n\")\n",
    "\n",
    "#\n",
    "#Se ejecuta funcion de Binning de clasificacion con arbol de decicion sobre resultados del prebinning\n",
    "#ini1 = time()\n",
    "Binning = pool.map(lambda t:  multiDecisionTree(t), clasificacion)\n",
    "#total1 = time() - ini1\n",
    "#print('Duracion Arbol de Decision: ', total1/60,' Minutos')\n",
    "#print(\"\\n\")\n",
    "\n",
    "#\n",
    "#Se ejecuta funcion que asigna la nueva clasificacion obtenida desde el arbol a la data de entrada\n",
    "#ini2 = time()\n",
    "limitBinning = pool.map(lambda p:  LimitCategBinning(p), Binning)\n",
    "#total2 = time() - ini2\n",
    "#print('Duracion LimitCategBinning: ', total2,' Segundos')\n",
    "#print(\"\\n\")\n",
    "\n",
    "#\n",
    "#Se ejecuta funcion que devuelve los limites originales al resultado del binning\n",
    "#ini5 = time()\n",
    "LimitesOriginales = pool.map(lambda l: limitesOriginales(l, limitCateg), limitBinning)\n",
    "#total5 = time() - ini5\n",
    "#print('Duracion limites originales: ', total5,' Segundos')\n",
    "#print(\"\\n\")\n",
    "\n",
    "#\n",
    "#Se ejecuta funcion que asigna la clasificacion y limites originales a la data de entrada\n",
    "#ini3 = time()\n",
    "New_Cat = pool.map(lambda p: MultiAsignacionCategorias(p, LimitesOriginales, dfVarsNum), preBinning)\n",
    "#total3 = time() - ini3\n",
    "#print('Duracion MultiAsignacionCategorias: ', total3,' Segundos')\n",
    "#print(\"\\n\")\n",
    "\n",
    "#\n",
    "#Se ejecuta la funcion que calcula la tabla IV WOE de cada variable Numerica\n",
    "#ini4 = time()\n",
    "IVWOE_Master = pool.map(lambda n: MultiIvWoe(n, data), New_Cat)\n",
    "#total4 = time() - ini4\n",
    "#print('Duracion creacion de tablas IV WOE: ', total4/60,' Minutos')\n",
    "#print(\"\\n\")\n",
    "#################################################################\n",
    "### Fin analisis de variables numericas                         #\n",
    "#################################################################\n",
    "#\n",
    "#\n",
    "#Se ejecuta la funcion que genera lista de variables categoricas\n",
    "#inic = time()\n",
    "ensambladorCat = pool.map(lambda e: ensambladorC(e, dataC, varDep), parametersC)\n",
    "BinningCateg = pool.map(lambda t:  multiDecisionTreeCateg(t), ensambladorCat)\n",
    "IVWOE_CAT = pool.map(lambda p:  MultiIvWoeCategorico(p), BinningCateg)\n",
    "#totalc = time() - inic\n",
    "#print('Duracion creacion de tablas IV WOE categorico: ', totalc,' Segundos')\n",
    "#print(\"\\n\")\n",
    "\n",
    "#################################################################\n",
    "### Fin analisis de variables categoricas                       #\n",
    "#################################################################\n",
    "\n",
    "totalfinal = time.time() - ini\n",
    "print('Duracion Total categorizacion_automatica: ', totalfinal/60 ,' Minutos')\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-cisco",
   "metadata": {},
   "source": [
    "#### Escritura en Datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini = time.time()\n",
    "##Limpieza de la tabla que estaba ingestada antes\n",
    "###\n",
    "vacio = IVWOE_Master[0].limit(0)\n",
    "vacio.write.mode(\"overwrite\").saveAsTable(\"grsg_modelos.IV_WOE_MAESTRA_\" + varDep)\n",
    "i=0\n",
    "ivwoe_completa = IVWOE_Master + IVWOE_CAT\n",
    "for x in ivwoe_completa:    \n",
    "    x.write.mode(\"append\").saveAsTable(\"grsg_modelos.IV_WOE_MAESTRA_\" + varDep)\n",
    "    i = i+1\n",
    "    clear_output(wait=True)\n",
    "    print(\"Se han escrito --->  \" + str(i)+\" variables\")\n",
    "total = time.time()-ini\n",
    "print('Tiempo de migracion tabla IV WOE Maestra ', total/60,' Minutos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-mixture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
